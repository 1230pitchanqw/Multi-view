{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import _pickle as pickle\n",
    "import random\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.349851\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "input_age_seq = pickle.load(open(\"../data/baseline/input_age_seq\",\"rb\"))\n",
    "input_sex_seq = pickle.load(open(\"../data/baseline/input_sex_seq\",\"rb\"))\n",
    "\n",
    "input_cost_seq = pickle.load(open(\"../data/baseline/input_cost_seq\",\"rb\"))\n",
    "target_cost_seq = pickle.load(open(\"../data/baseline/target_cost_seq\",\"rb\"))\n",
    "\n",
    "input_medical_cost_seq = pickle.load(open(\"../data/baseline/input_medical_cost_seq\",\"rb\"))\n",
    "input_monthly_medical_cost_seq = pickle.load(open(\"../data/baseline/input_monthly_medical_cost_seq\",\"rb\"))\n",
    "\n",
    "input_pharmacy_cost_seq = pickle.load(open(\"../data/baseline/input_pharmacy_cost_seq\",\"rb\"))\n",
    "input_monthly_pharmacy_cost_seq = pickle.load(open(\"../data/baseline/input_monthly_pharmacy_cost_seq\",\"rb\"))\n",
    "\n",
    "print(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(seq, size):    \n",
    "    X = np.zeros((len(seq), size))\n",
    "    for i in range(len(seq)):\n",
    "        value = seq[i]\n",
    "        X[i][value] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_feature = build_feature([i//5 for i in input_age_seq], 4)\n",
    "sex_feature = build_feature([1 if i==\"M\" else 0 for i in input_sex_seq], 2)\n",
    "\n",
    "cost_feature = np.array([np.log(i+1) for i in input_cost_seq]).reshape(-1,1)\n",
    "medical_cost_feature = np.array([np.log(i+1) for i in input_medical_cost_seq]).reshape(-1,1)\n",
    "pharmacy_cost_feature = np.array([np.log(i+1) for i in input_pharmacy_cost_seq]).reshape(-1,1)\n",
    "\n",
    "monthly_medical_cost_feature = np.array([[np.log(i+1) for i in y] for y in input_monthly_medical_cost_seq]).reshape(-1,12)\n",
    "monthly_pharmacy_cost_feature = np.array([[np.log(i+1) for i in y] for y in input_monthly_pharmacy_cost_seq]).reshape(-1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143102, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [age_feature, sex_feature, cost_feature, medical_cost_feature, pharmacy_cost_feature]\n",
    "\n",
    "X_cost = np.concatenate(features, axis =1)\n",
    "X_cost.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([np.log(i+1) for i in target_cost_seq])\n",
    "# y = np.array([x/len(target_cost_seq) for x in ss.rankdata(target_cost_seq)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:06.072725\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "input_util_seq = pickle.load(open(\"../data/advance/input_util_seq\",\"rb\"))\n",
    "\n",
    "print(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for p in input_util_seq:\n",
    "    for v in p:\n",
    "        for c in v:\n",
    "            if c not in vocab: vocab[c] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq_feature(seq, vocab):\n",
    "    X = np.zeros((len(seq),12, len(vocab) ))\n",
    "    for i in range(len(seq)):\n",
    "        for j in range(12):\n",
    "            for value in seq[i][j]:\n",
    "                X[i][j][vocab[value]] +=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_util = build_seq_feature(input_util_seq, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_util = np.concatenate((monthly_medical_cost_feature.reshape(-1,12,1),\\\n",
    "                         monthly_pharmacy_cost_feature.reshape(-1,12,1),\\\n",
    "                         X_util), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143102, 12, 38)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_util.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:11.213560\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "input_diag_seq = pickle.load(open(\"../data/advance/input_diag_seq\",\"rb\"))\n",
    "input_proc_seq = pickle.load(open(\"../data/advance/input_proc_seq\",\"rb\"))\n",
    "input_drug_seq = pickle.load(open(\"../data/advance/input_drug_seq\",\"rb\"))\n",
    "\n",
    "print(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10918"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2int = {\"PAD\":0}\n",
    "\n",
    "code_seq = []\n",
    "for p_diag, p_proc, p_drug in zip(input_diag_seq, input_proc_seq, input_drug_seq):\n",
    "    new_p = []\n",
    "    for diag, proc, drug in zip(p_diag, p_proc, p_drug):\n",
    "        new_v = []\n",
    "        for d in diag:\n",
    "            if d not in code2int: code2int[d] = len(code2int)\n",
    "            new_v.append(code2int[d])\n",
    "        for p in proc:\n",
    "            if p not in code2int: code2int[p] = len(code2int)\n",
    "            new_v.append(code2int[p])\n",
    "        for dr in drug:\n",
    "            if dr not in code2int: code2int[dr] = len(code2int)\n",
    "            new_v.append(code2int[dr])\n",
    "        new_p.append(new_v)\n",
    "    code_seq.append(new_p)\n",
    "    \n",
    "len(code2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq(seq, max_codes = 50, max_length=12):\n",
    "    X = np.zeros((len(seq), max_length, max_codes))\n",
    "    for i, p in enumerate(seq):\n",
    "        assert len(p) == max_length\n",
    "        for j, claim in enumerate(p):\n",
    "            claim = claim[:max_codes]\n",
    "            X[i][j][:len(claim)] = claim\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143102, 12, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_code = build_seq(code_seq)\n",
    "X_code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
    "                                     shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c, a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len =12,\n",
    "                max_code=50,\n",
    "                max_util=38,\n",
    "                max_demo=9,\n",
    "                feature_code=len(code2int),\n",
    "                embed_dim = 50,\n",
    "                lstm_units=32,\n",
    "               ):\n",
    "    \n",
    "    input_code = layers.Input(shape=(max_len, max_code))\n",
    "    input_util = layers.Input(shape=(max_len, max_util))\n",
    "    input_demo = layers.Input(shape=(max_demo,))\n",
    "    inputs_list = [input_code, input_util, input_demo]\n",
    "    \n",
    "    # code\n",
    "    tmp_input = layers.Input(shape=(max_code, ))\n",
    "    tmp = layers.Embedding(input_dim=feature_code, output_dim=embed_dim, \n",
    "                           mask_zero=True, name='code_embedding')(tmp_input)\n",
    "    \n",
    "    tmp = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(tmp)\n",
    "    tmp, ait = Attention(max_code)(tmp)\n",
    "    codeEncoder = keras.models.Model(tmp_input, tmp)    \n",
    "    \n",
    "    input_code = layers.TimeDistributed(codeEncoder)(input_code)\n",
    "    input_code = layers.Bidirectional(layers.GRU(lstm_units, return_sequences=True))(input_code)\n",
    "    input_code, ait2 = Attention(max_len)(input_code)\n",
    "        \n",
    "    # util\n",
    "    input_util = layers.Dense(lstm_units, activation=\"relu\")(input_util)\n",
    "    \n",
    "    input_util = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(input_util)\n",
    "    input_util, ait3 = Attention(max_len)(input_util)\n",
    "    \n",
    "    # demo\n",
    "    out = layers.concatenate([input_code, input_util, input_demo])\n",
    "    \n",
    "    out = layers.Dense(lstm_units, activation=\"relu\")(out)\n",
    "    out = layers.Dropout(0.5)(out)\n",
    "\n",
    "    out = layers.Dense(lstm_units, activation=\"relu\")(out)\n",
    "    out = layers.Dropout(0.5)(out)\n",
    "    \n",
    "    out = layers.Dense(1, activation=None, name='main_output')(out)\n",
    "    model = keras.models.Model(inputs=inputs_list, outputs=[out])\n",
    "\n",
    "    model.compile(optimizer='adam', loss=\"mse\")\n",
    "#     print(model.summary())\n",
    "#     print(codeEncoder.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(y_true, y_pred):\n",
    "    return metrics.mean_absolute_error(y_true, y_pred), \\\n",
    "           metrics.r2_score(y_true, y_pred),\\\n",
    "           np.sqrt(metrics.mean_squared_error(y_true, y_pred)),\\\n",
    "           scipy.stats.pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "def generate_result(seed):\n",
    "    model = build_model()\n",
    "    idx_train, idx_val = train_test_split(range(len(y)), train_size=0.85, random_state=seed)\n",
    "    idx_train, idx_test = train_test_split(range(len(idx_train)), train_size=0.82, random_state=seed)\n",
    "\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='min', restore_best_weights=True)\n",
    "    history = model.fit([X_code[idx_train], X_util[idx_train], X_cost[idx_train]], y[idx_train], epochs=50, batch_size=100, \\\n",
    "                        validation_data=([X_code[idx_val], X_util[idx_val], X_cost[idx_val]], y[idx_val]), verbose=1, callbacks=[earlyStopping])\n",
    "\n",
    "\n",
    "    y_pred = model.predict([X_code[idx_test], X_util[idx_test], X_cost[idx_test]], verbose=0).reshape(-1)\n",
    "    mae, r2, rmse, pcc = result(y[idx_test], y_pred)\n",
    "    return mae, r2, rmse, pcc, y_pred\n",
    "\n",
    "def display(list_eva):\n",
    "    for list_ in list_eva:\n",
    "        print(np.mean(list_), np.std(list_))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/50\n",
      "998/998 [==============================] - 702s 703ms/step - loss: 6.4518 - val_loss: 2.7454\n",
      "Epoch 2/50\n",
      "998/998 [==============================] - 664s 666ms/step - loss: 4.3538 - val_loss: 2.7529\n",
      "Epoch 3/50\n",
      "998/998 [==============================] - 644s 645ms/step - loss: 3.8242 - val_loss: 2.6899\n",
      "Epoch 4/50\n",
      "998/998 [==============================] - 644s 645ms/step - loss: 3.5557 - val_loss: 2.8653\n",
      "Epoch 5/50\n",
      "998/998 [==============================] - 643s 645ms/step - loss: 3.2931 - val_loss: 2.5707\n",
      "Epoch 6/50\n",
      "998/998 [==============================] - 640s 641ms/step - loss: 3.0969 - val_loss: 2.5961\n",
      "Epoch 7/50\n",
      "998/998 [==============================] - 642s 643ms/step - loss: 2.9190 - val_loss: 2.5557\n",
      "Epoch 8/50\n",
      "998/998 [==============================] - 644s 645ms/step - loss: 2.7916 - val_loss: 2.5360\n",
      "Epoch 9/50\n",
      "998/998 [==============================] - 641s 642ms/step - loss: 2.6665 - val_loss: 2.5526\n",
      "Epoch 10/50\n",
      "998/998 [==============================] - 645s 646ms/step - loss: 2.5575 - val_loss: 2.5222\n",
      "Epoch 11/50\n",
      "998/998 [==============================] - 645s 646ms/step - loss: 2.4888 - val_loss: 2.4748\n",
      "Epoch 12/50\n",
      "998/998 [==============================] - 644s 645ms/step - loss: 2.4444 - val_loss: 2.4738\n",
      "Epoch 13/50\n",
      "998/998 [==============================] - 640s 641ms/step - loss: 2.3931 - val_loss: 2.4567\n",
      "Epoch 14/50\n",
      "998/998 [==============================] - 641s 643ms/step - loss: 2.3623 - val_loss: 2.4269\n",
      "Epoch 15/50\n",
      "998/998 [==============================] - 612s 613ms/step - loss: 2.3266 - val_loss: 2.4163\n",
      "Epoch 16/50\n",
      "998/998 [==============================] - 359s 360ms/step - loss: 2.2963 - val_loss: 2.4217\n",
      "Epoch 17/50\n",
      "998/998 [==============================] - 357s 357ms/step - loss: 2.2755 - val_loss: 2.4131\n",
      "Epoch 18/50\n",
      "998/998 [==============================] - 355s 356ms/step - loss: 2.2550 - val_loss: 2.3956\n",
      "Epoch 19/50\n",
      "998/998 [==============================] - 356s 357ms/step - loss: 2.2246 - val_loss: 2.3912\n",
      "Epoch 20/50\n",
      "998/998 [==============================] - 356s 357ms/step - loss: 2.2052 - val_loss: 2.3874\n",
      "Epoch 21/50\n",
      "998/998 [==============================] - 368s 369ms/step - loss: 2.1885 - val_loss: 2.3767\n",
      "Epoch 22/50\n",
      "998/998 [==============================] - 348s 349ms/step - loss: 2.1680 - val_loss: 2.3943\n",
      "Epoch 23/50\n",
      "998/998 [==============================] - 350s 351ms/step - loss: 2.1444 - val_loss: 2.3673\n",
      "Epoch 24/50\n",
      "998/998 [==============================] - 350s 351ms/step - loss: 2.1461 - val_loss: 2.3380\n",
      "Epoch 25/50\n",
      "998/998 [==============================] - 348s 349ms/step - loss: 2.1119 - val_loss: 2.3769\n",
      "Epoch 26/50\n",
      "998/998 [==============================] - 345s 346ms/step - loss: 2.0956 - val_loss: 2.3227\n",
      "Epoch 27/50\n",
      "998/998 [==============================] - 348s 349ms/step - loss: 2.0735 - val_loss: 2.3285\n",
      "Epoch 28/50\n",
      "998/998 [==============================] - 349s 350ms/step - loss: 2.0632 - val_loss: 2.3404\n",
      "1\n",
      "Epoch 1/50\n",
      "998/998 [==============================] - 366s 367ms/step - loss: 7.8733 - val_loss: 2.9251\n",
      "Epoch 2/50\n",
      "998/998 [==============================] - 357s 358ms/step - loss: 4.7787 - val_loss: 2.6773\n",
      "Epoch 3/50\n",
      "998/998 [==============================] - 359s 360ms/step - loss: 4.2070 - val_loss: 2.7147\n",
      "Epoch 4/50\n",
      "998/998 [==============================] - 360s 360ms/step - loss: 3.8374 - val_loss: 2.5526\n",
      "Epoch 5/50\n",
      "998/998 [==============================] - 359s 359ms/step - loss: 3.4664 - val_loss: 2.4899\n",
      "Epoch 6/50\n",
      "998/998 [==============================] - 360s 361ms/step - loss: 3.1652 - val_loss: 2.5204\n",
      "Epoch 7/50\n",
      "998/998 [==============================] - 360s 360ms/step - loss: 2.9159 - val_loss: 2.4272\n",
      "Epoch 8/50\n",
      "998/998 [==============================] - 358s 359ms/step - loss: 2.7324 - val_loss: 2.4679\n",
      "Epoch 9/50\n",
      "998/998 [==============================] - 360s 360ms/step - loss: 2.5936 - val_loss: 2.4395\n",
      "2\n",
      "Epoch 1/50\n",
      "998/998 [==============================] - 366s 367ms/step - loss: 6.7994 - val_loss: 3.1094\n",
      "Epoch 2/50\n",
      "998/998 [==============================] - 363s 364ms/step - loss: 4.3863 - val_loss: 2.7592\n",
      "Epoch 3/50\n",
      "998/998 [==============================] - 356s 357ms/step - loss: 3.7759 - val_loss: 2.6570\n",
      "Epoch 4/50\n",
      "998/998 [==============================] - 352s 353ms/step - loss: 3.5003 - val_loss: 2.5341\n",
      "Epoch 5/50\n",
      "998/998 [==============================] - 353s 354ms/step - loss: 3.2377 - val_loss: 2.5655\n",
      "Epoch 6/50\n",
      "998/998 [==============================] - 354s 355ms/step - loss: 3.0321 - val_loss: 2.5289\n",
      "Epoch 7/50\n",
      "998/998 [==============================] - 352s 352ms/step - loss: 2.8616 - val_loss: 2.4459\n",
      "Epoch 8/50\n",
      "998/998 [==============================] - 352s 352ms/step - loss: 2.7295 - val_loss: 2.4491\n",
      "Epoch 9/50\n",
      "998/998 [==============================] - 354s 355ms/step - loss: 2.6044 - val_loss: 2.4368\n",
      "Epoch 10/50\n",
      "998/998 [==============================] - 355s 355ms/step - loss: 2.5200 - val_loss: 2.4862\n",
      "Epoch 11/50\n",
      "998/998 [==============================] - 355s 356ms/step - loss: 2.4486 - val_loss: 2.3973\n",
      "Epoch 12/50\n",
      "998/998 [==============================] - 357s 358ms/step - loss: 2.3875 - val_loss: 2.3878\n",
      "Epoch 13/50\n",
      "998/998 [==============================] - 357s 357ms/step - loss: 2.3461 - val_loss: 2.4130\n",
      "Epoch 14/50\n",
      "998/998 [==============================] - 358s 358ms/step - loss: 2.3173 - val_loss: 2.3984\n",
      "3\n",
      "Epoch 1/50\n",
      "998/998 [==============================] - 369s 369ms/step - loss: 6.8935 - val_loss: 3.3362\n",
      "Epoch 2/50\n",
      "998/998 [==============================] - 361s 362ms/step - loss: 4.5542 - val_loss: 2.7050\n",
      "Epoch 3/50\n",
      "998/998 [==============================] - 359s 360ms/step - loss: 3.9458 - val_loss: 2.5969\n",
      "Epoch 4/50\n",
      "998/998 [==============================] - 362s 363ms/step - loss: 3.5833 - val_loss: 2.5550\n",
      "Epoch 5/50\n",
      "998/998 [==============================] - 363s 364ms/step - loss: 3.2499 - val_loss: 2.4995\n",
      "Epoch 6/50\n",
      "998/998 [==============================] - 361s 362ms/step - loss: 3.0372 - val_loss: 2.4807\n",
      "Epoch 7/50\n",
      "998/998 [==============================] - 362s 363ms/step - loss: 2.8182 - val_loss: 2.4636\n",
      "Epoch 8/50\n",
      "998/998 [==============================] - 364s 365ms/step - loss: 2.6682 - val_loss: 2.4523\n",
      "Epoch 9/50\n",
      "998/998 [==============================] - 360s 361ms/step - loss: 2.5492 - val_loss: 2.4720\n",
      "Epoch 10/50\n",
      "998/998 [==============================] - 360s 361ms/step - loss: 2.4682 - val_loss: 2.4639\n",
      "4\n",
      "Epoch 1/50\n",
      "998/998 [==============================] - 374s 374ms/step - loss: 6.7263 - val_loss: 2.7446\n",
      "Epoch 2/50\n",
      "998/998 [==============================] - 370s 370ms/step - loss: 4.3905 - val_loss: 2.6451\n",
      "Epoch 3/50\n",
      "998/998 [==============================] - 356s 357ms/step - loss: 3.8690 - val_loss: 2.5895\n",
      "Epoch 4/50\n",
      "998/998 [==============================] - 365s 366ms/step - loss: 3.5621 - val_loss: 2.6565\n",
      "Epoch 5/50\n",
      "998/998 [==============================] - 364s 364ms/step - loss: 3.2939 - val_loss: 2.5170\n",
      "Epoch 6/50\n",
      "998/998 [==============================] - 363s 364ms/step - loss: 3.0519 - val_loss: 2.4598\n",
      "Epoch 7/50\n",
      "998/998 [==============================] - 362s 362ms/step - loss: 2.8531 - val_loss: 2.4634\n",
      "Epoch 8/50\n",
      "998/998 [==============================] - 356s 357ms/step - loss: 2.6993 - val_loss: 2.4717\n",
      "1.064089097167655 0.021079208865460854\n",
      "\n",
      "0.2985478546112244 0.016480561804507413\n",
      "\n",
      "1.5999288836465517 0.03183996693341982\n",
      "\n",
      "0.5493606106513074 0.013647227027621021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mae_list, r2_list, rmse_list, pcc_list = [], [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    mae, r2, rmse, pcc, y_pred = generate_result(seed=i)\n",
    "    mae_list.append(mae)\n",
    "    r2_list.append(r2)\n",
    "    rmse_list.append(rmse)\n",
    "    pcc_list.append(pcc)\n",
    "    \n",
    "display([mae_list, r2_list, rmse_list, pcc_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
